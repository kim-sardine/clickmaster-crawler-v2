#!/usr/bin/env python3
"""
ì™„ë£Œëœ ë°°ì¹˜ ê²°ê³¼ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
ì™„ë£Œëœ ë°°ì¹˜ì˜ ê²°ê³¼ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë‚šì‹œì„± ì ìˆ˜ë¥¼ DBì— ì—…ë°ì´íŠ¸
"""

import sys
import os
import json
import logging
from typing import List, Dict, Any, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.config.settings import Settings
from src.database.operations import NewsOperations, BatchOperations
from src.models.batch_status import BatchStatus
from src.models.base import AnswerFormat
from src.utils.logging_utils import (
    setup_logger,
    log_function_start,
    log_function_end,
    log_error,
    log_batch_status,
    log_progress,
)
from src.utils.file_utils import create_temp_file, delete_file_safely
import openai

logger = setup_logger(__name__)


class BatchProcessor:
    """ë°°ì¹˜ ê²°ê³¼ ì²˜ë¦¬ê¸°"""

    def __init__(self):
        openai.api_key = Settings.OPENAI_API_KEY
        self.client = openai.OpenAI()

    def download_batch_results(self, batch_id: str, output_file_id: str) -> Optional[str]:
        """ë°°ì¹˜ ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            # OpenAIì—ì„œ ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            file_response = self.client.files.content(output_file_id)

            # ì„ì‹œ íŒŒì¼ì— ì €ì¥
            temp_file = create_temp_file(suffix=".jsonl", prefix=f"batch_output_{batch_id}_")

            with open(temp_file, "wb") as f:
                f.write(file_response.content)

            logger.info(f"ğŸ“¥ Downloaded batch results: {output_file_id} -> {temp_file}")
            return temp_file

        except Exception as e:
            log_error(logger, e, f"downloading batch results: {batch_id}")
            return None

    def parse_batch_response(self, response_line: str) -> Optional[Dict[str, Any]]:
        """ë°°ì¹˜ ì‘ë‹µ ë¼ì¸ íŒŒì‹±"""
        try:
            response_data = json.loads(response_line.strip())

            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            custom_id = response_data.get("custom_id", "")
            news_id = custom_id.replace("news_", "") if custom_id.startswith("news_") else None

            if not news_id:
                logger.warning(f"âš ï¸ Invalid custom_id: {custom_id}")
                return None

            # ì‘ë‹µ ë‚´ìš© í™•ì¸
            response_obj = response_data.get("response")
            if not response_obj:
                logger.warning(f"âš ï¸ No response object for news_id: {news_id}")
                return None

            # ì—ëŸ¬ í™•ì¸
            if response_obj.get("status_code") != 200:
                error_msg = response_obj.get("body", {}).get("error", {}).get("message", "Unknown error")
                logger.warning(f"âš ï¸ API error for news_id {news_id}: {error_msg}")
                return None

            # ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ
            choices = response_obj.get("body", {}).get("choices", [])
            if not choices:
                logger.warning(f"âš ï¸ No choices for news_id: {news_id}")
                return None

            message_content = choices[0].get("message", {}).get("content", "")
            if not message_content:
                logger.warning(f"âš ï¸ No message content for news_id: {news_id}")
                return None

            # JSON ì‘ë‹µ íŒŒì‹±
            try:
                ai_response = json.loads(message_content)

                # AnswerFormat ê²€ì¦
                answer = AnswerFormat(
                    clickbait_score=ai_response["clickbait_score"], reasoning=ai_response["reasoning"]
                )

                return {"id": int(news_id), "clickbait_score": answer.clickbait_score, "reasoning": answer.reasoning}

            except (json.JSONDecodeError, KeyError, ValueError) as e:
                logger.warning(f"âš ï¸ Failed to parse AI response for news_id {news_id}: {str(e)}")
                logger.debug(f"Raw content: {message_content}")
                return None

        except Exception as e:
            log_error(logger, e, f"parsing batch response line")
            return None

    def process_batch_file(self, file_path: str) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ê²°ê³¼ íŒŒì¼ ì²˜ë¦¬"""
        try:
            score_updates = []

            with open(file_path, "r", encoding="utf-8") as f:
                lines = f.readlines()

            logger.info(f"ğŸ“Š Processing {len(lines)} batch responses...")

            for i, line in enumerate(lines):
                if not line.strip():
                    continue

                result = self.parse_batch_response(line)
                if result:
                    score_updates.append(result)

                # ì§„í–‰ë¥  ë¡œê·¸ (100ê°œë§ˆë‹¤)
                if (i + 1) % 100 == 0:
                    log_progress(logger, i + 1, len(lines), "Processing responses")

            success_rate = len(score_updates) / len(lines) * 100 if lines else 0
            logger.info(f"âœ… Successfully processed {len(score_updates)}/{len(lines)} responses ({success_rate:.1f}%)")

            return score_updates

        except Exception as e:
            log_error(logger, e, f"processing batch file: {file_path}")
            return []

    def process_completed_batch(self, batch_info: Dict[str, Any]) -> bool:
        """ì™„ë£Œëœ ë°°ì¹˜ ì²˜ë¦¬"""
        try:
            batch_id = batch_info["batch_id"]
            output_file_id = batch_info["output_file_id"]

            log_batch_status(logger, batch_id, "processing", "Downloading results")

            # ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            result_file = self.download_batch_results(batch_id, output_file_id)
            if not result_file:
                return False

            try:
                # ê²°ê³¼ íŒŒì¼ ì²˜ë¦¬
                score_updates = self.process_batch_file(result_file)

                if not score_updates:
                    logger.warning(f"âš ï¸ No valid score updates from batch {batch_id}")
                    return False

                # ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
                log_batch_status(logger, batch_id, "updating", f"Updating {len(score_updates)} scores")

                updated_count = NewsOperations.update_clickbait_scores(score_updates)

                # ë°°ì¹˜ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì™„ë£Œë¡œ í‘œì‹œí•˜ê³  ì²˜ë¦¬ëœ ìˆ˜ ê¸°ë¡)
                BatchOperations.update_batch_status(
                    batch_id, BatchStatus.COMPLETED.value, processed_count=updated_count
                )

                log_batch_status(logger, batch_id, "completed", f"Updated {updated_count} articles")

                return True

            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                delete_file_safely(result_file)

        except Exception as e:
            log_error(logger, e, f"processing completed batch: {batch_info.get('batch_id', 'Unknown')}")

            # ì—ëŸ¬ ë°œìƒ ì‹œ ë°°ì¹˜ ìƒíƒœë¥¼ ì‹¤íŒ¨ë¡œ ì—…ë°ì´íŠ¸
            try:
                BatchOperations.update_batch_status(
                    batch_info["batch_id"], BatchStatus.FAILED.value, error_message=str(e)
                )
            except:
                pass

            return False

    def get_batch_processing_stats(self) -> Dict[str, int]:
        """ë°°ì¹˜ ì²˜ë¦¬ í†µê³„"""
        try:
            # ì™„ë£Œëœ ë°°ì¹˜ ìˆ˜
            completed_batches = BatchOperations.get_completed_batches()

            # ì „ì²´ í†µê³„
            stats = NewsOperations.get_news_stats()

            return {
                "completed_batches_to_process": len(completed_batches),
                "total_news": stats.get("total", 0),
                "processed_news": stats.get("processed", 0),
                "unprocessed_news": stats.get("unprocessed", 0),
            }

        except Exception as e:
            log_error(logger, e, "getting batch processing stats")
            return {}


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        log_function_start(logger, "process_completed_batches")

        # í™˜ê²½ë³€ìˆ˜ ê²€ì¦
        Settings.validate_required_env_vars()

        processor = BatchProcessor()

        # ì´ˆê¸° í†µê³„
        initial_stats = processor.get_batch_processing_stats()
        logger.info(f"ğŸ“Š Initial stats: {initial_stats}")

        # ì™„ë£Œëœ ë°°ì¹˜ ì¡°íšŒ
        completed_batches = BatchOperations.get_completed_batches()

        if not completed_batches:
            logger.info("âœ… No completed batches to process")
            log_function_end(logger, "process_completed_batches")
            return

        logger.info(f"ğŸ“‹ Found {len(completed_batches)} completed batches to process")

        # ê° ë°°ì¹˜ ì²˜ë¦¬
        success_count = 0
        for i, batch_info in enumerate(completed_batches):
            batch_id = batch_info["batch_id"]

            try:
                logger.info(f"ğŸ”„ Processing batch {i + 1}/{len(completed_batches)}: {batch_id}")

                success = processor.process_completed_batch(batch_info)
                if success:
                    success_count += 1
                    logger.info(f"âœ… Successfully processed batch: {batch_id}")
                else:
                    logger.warning(f"âš ï¸ Failed to process batch: {batch_id}")

            except Exception as e:
                log_error(logger, e, f"processing batch: {batch_id}")
                continue

        # ìµœì¢… í†µê³„
        final_stats = processor.get_batch_processing_stats()
        logger.info(f"ğŸ“Š Processing Summary:")
        logger.info(f"   Batches processed: {success_count}/{len(completed_batches)}")
        logger.info(f"   Final stats: {final_stats}")

        log_function_end(logger, "process_completed_batches")

    except Exception as e:
        log_error(logger, e, "main process_completed_batches function")
        sys.exit(1)


if __name__ == "__main__":
    main()
