#!/usr/bin/env python3
"""
íŠ¹ë³„ ë‰´ìŠ¤ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ (Naksi King)
íŠ¹ì • ì¡°ê±´ì˜ ë‰´ìŠ¤ë‚˜ íŠ¹ë³„í•œ ì²˜ë¦¬ê°€ í•„ìš”í•œ ë‰´ìŠ¤ë¥¼ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.config.settings import Settings
from src.config.keywords import get_keywords_by_category
from src.database.operations import NewsOperations, BatchOperations
from src.models.batch_status import BatchStatus
from src.utils.logging_utils import setup_logger, log_function_start, log_function_end, log_error, log_news_stats
from src.utils.date_utils import get_kst_now, get_date_range_for_days_ago
from src.utils.text_utils import detect_clickbait_patterns

logger = setup_logger(__name__)


class NaksiKingProcessor:
    """íŠ¹ë³„ ë‰´ìŠ¤ ì²˜ë¦¬ê¸° (ë‚šì‹œì™• ì „ìš©)"""

    def __init__(self):
        self.high_score_threshold = 8.0  # ë†’ì€ ë‚šì‹œì„± ì ìˆ˜ ê¸°ì¤€
        self.special_keywords = [
            "ì¶©ê²©",
            "ê¹œì§",
            "ëŒ€ë°•",
            "ì‹¤í™”",
            "ë†€ë¼ìš´",
            "ë¯¿ì„ ìˆ˜ ì—†ëŠ”",
            "ê²½ì•…",
            "í™”ì œ",
            "ë…¼ë€",
            "í­ë¡œ",
            "ê³ ë°±",
            "ë¹„ë°€",
        ]

    def find_high_clickbait_news(self, days_back: int = 7) -> List[Dict[str, Any]]:
        """ë†’ì€ ë‚šì‹œì„± ì ìˆ˜ì˜ ë‰´ìŠ¤ ì°¾ê¸°"""
        try:
            # ìµœê·¼ Nì¼ê°„ì˜ ë†’ì€ ì ìˆ˜ ë‰´ìŠ¤ ì¡°íšŒ
            start_date, end_date = get_date_range_for_days_ago(days_back)

            # Supabaseì—ì„œ ë†’ì€ ì ìˆ˜ ë‰´ìŠ¤ ì¡°íšŒ (ì„ì‹œë¡œ ì§ì ‘ ì¿¼ë¦¬)
            from src.database.supabase_client import supabase_client

            result = (
                supabase_client.client.table("articles")
                .select("*")
                .gte("clickbait_score", self.high_score_threshold)
                .gte("published_date", start_date)
                .lte("published_date", end_date)
                .order("clickbait_score", desc=True)
                .execute()
            )

            high_score_news = result.data
            logger.info(
                f"ğŸ¯ Found {len(high_score_news)} high clickbait articles (score >= {self.high_score_threshold})"
            )

            return high_score_news

        except Exception as e:
            log_error(logger, e, "finding high clickbait news")
            return []

    def analyze_clickbait_patterns(self, news_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë‚šì‹œì„± íŒ¨í„´ ë¶„ì„"""
        try:
            pattern_stats = {}
            keyword_stats = {}
            source_stats = {}
            score_distribution = {"8.0-8.5": 0, "8.5-9.0": 0, "9.0-9.5": 0, "9.5-10.0": 0}

            for news in news_list:
                title = news.get("title", "")
                source = news.get("source", "Unknown")
                score = news.get("clickbait_score", 0)

                # íŒ¨í„´ ë¶„ì„
                patterns = detect_clickbait_patterns(title)
                for pattern in patterns:
                    pattern_stats[pattern] = pattern_stats.get(pattern, 0) + 1

                # í‚¤ì›Œë“œ ë¶„ì„
                for keyword in self.special_keywords:
                    if keyword in title:
                        keyword_stats[keyword] = keyword_stats.get(keyword, 0) + 1

                # ì¶œì²˜ë³„ í†µê³„
                source_stats[source] = source_stats.get(source, 0) + 1

                # ì ìˆ˜ ë¶„í¬
                if 8.0 <= score < 8.5:
                    score_distribution["8.0-8.5"] += 1
                elif 8.5 <= score < 9.0:
                    score_distribution["8.5-9.0"] += 1
                elif 9.0 <= score < 9.5:
                    score_distribution["9.0-9.5"] += 1
                elif 9.5 <= score <= 10.0:
                    score_distribution["9.5-10.0"] += 1

            analysis = {
                "total_analyzed": len(news_list),
                "pattern_stats": pattern_stats,
                "keyword_stats": keyword_stats,
                "source_stats": source_stats,
                "score_distribution": score_distribution,
                "analysis_date": get_kst_now().isoformat(),
            }

            return analysis

        except Exception as e:
            log_error(logger, e, "analyzing clickbait patterns")
            return {}

    def generate_naksi_king_report(self, analysis: Dict[str, Any]) -> str:
        """ë‚šì‹œì™• ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            report_lines = []
            report_lines.append("ğŸ£ === NAKSI KING REPORT ===")
            report_lines.append(f"ğŸ“… Analysis Date: {analysis.get('analysis_date', 'Unknown')}")
            report_lines.append(f"ğŸ“Š Total High-Clickbait Articles: {analysis.get('total_analyzed', 0)}")
            report_lines.append("")

            # ì ìˆ˜ ë¶„í¬
            report_lines.append("ğŸ“ˆ Score Distribution:")
            score_dist = analysis.get("score_distribution", {})
            for score_range, count in score_dist.items():
                if count > 0:
                    report_lines.append(f"   {score_range}: {count} articles")
            report_lines.append("")

            # ìƒìœ„ íŒ¨í„´
            report_lines.append("ğŸ¯ Top Clickbait Patterns:")
            pattern_stats = analysis.get("pattern_stats", {})
            sorted_patterns = sorted(pattern_stats.items(), key=lambda x: x[1], reverse=True)
            for pattern, count in sorted_patterns[:10]:
                report_lines.append(f"   {pattern}: {count}")
            report_lines.append("")

            # ìƒìœ„ í‚¤ì›Œë“œ
            report_lines.append("ğŸ”¥ Top Clickbait Keywords:")
            keyword_stats = analysis.get("keyword_stats", {})
            sorted_keywords = sorted(keyword_stats.items(), key=lambda x: x[1], reverse=True)
            for keyword, count in sorted_keywords[:10]:
                report_lines.append(f"   {keyword}: {count}")
            report_lines.append("")

            # ìƒìœ„ ì¶œì²˜
            report_lines.append("ğŸ“° Top Sources:")
            source_stats = analysis.get("source_stats", {})
            sorted_sources = sorted(source_stats.items(), key=lambda x: x[1], reverse=True)
            for source, count in sorted_sources[:10]:
                report_lines.append(f"   {source}: {count}")

            report_lines.append("\nğŸ£ === END OF REPORT ===")

            return "\n".join(report_lines)

        except Exception as e:
            log_error(logger, e, "generating naksi king report")
            return "Failed to generate report"

    def find_potential_naksi_king_candidates(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë‚šì‹œì™• í›„ë³´ ì°¾ê¸° (ì ìˆ˜ 9.5 ì´ìƒ)"""
        try:
            candidates = []

            for news in news_list:
                score = news.get("clickbait_score", 0)
                if score >= 9.5:
                    # ì¶”ê°€ ì •ë³´ í¬í•¨
                    candidate = {
                        "id": news.get("id"),
                        "title": news.get("title"),
                        "source": news.get("source"),
                        "clickbait_score": score,
                        "reasoning": news.get("reasoning"),
                        "published_date": news.get("published_date"),
                        "url": news.get("url"),
                    }
                    candidates.append(candidate)

            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            candidates.sort(key=lambda x: x["clickbait_score"], reverse=True)

            logger.info(f"ğŸ‘‘ Found {len(candidates)} Naksi King candidates (score >= 9.5)")

            return candidates

        except Exception as e:
            log_error(logger, e, "finding naksi king candidates")
            return []

    def update_naksi_king_status(self, candidates: List[Dict[str, Any]]) -> int:
        """ë‚šì‹œì™• ìƒíƒœ ì—…ë°ì´íŠ¸ (í–¥í›„ í™•ì¥ìš©)"""
        try:
            # í˜„ì¬ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³ , í–¥í›„ ë³„ë„ í…Œì´ë¸”ì´ë‚˜ í”Œë˜ê·¸ ì¶”ê°€ ê°€ëŠ¥
            updated_count = 0

            for candidate in candidates:
                logger.info(f"ğŸ‘‘ NAKSI KING: {candidate['title']} (Score: {candidate['clickbait_score']})")
                updated_count += 1

            return updated_count

        except Exception as e:
            log_error(logger, e, "updating naksi king status")
            return 0


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        log_function_start(logger, "process_naksi_king")

        # í™˜ê²½ë³€ìˆ˜ ê²€ì¦
        Settings.validate_required_env_vars()

        processor = NaksiKingProcessor()

        # 1. ë†’ì€ ë‚šì‹œì„± ì ìˆ˜ì˜ ë‰´ìŠ¤ ì°¾ê¸°
        logger.info("ğŸ” Finding high clickbait news...")
        high_score_news = processor.find_high_clickbait_news(days_back=7)

        if not high_score_news:
            logger.info("âœ… No high clickbait news found in the last 7 days")
            log_function_end(logger, "process_naksi_king")
            return

        # 2. ë‚šì‹œì„± íŒ¨í„´ ë¶„ì„
        logger.info("ğŸ“Š Analyzing clickbait patterns...")
        analysis = processor.analyze_clickbait_patterns(high_score_news)

        # 3. ë¦¬í¬íŠ¸ ìƒì„± ë° ì¶œë ¥
        logger.info("ğŸ“ Generating Naksi King report...")
        report = processor.generate_naksi_king_report(analysis)

        # ë¦¬í¬íŠ¸ ì¶œë ¥
        print("\n" + "=" * 60)
        print(report)
        print("=" * 60 + "\n")

        # 4. ë‚šì‹œì™• í›„ë³´ ì°¾ê¸°
        logger.info("ğŸ‘‘ Finding Naksi King candidates...")
        candidates = processor.find_potential_naksi_king_candidates(high_score_news)

        if candidates:
            logger.info(f"ğŸ‘‘ Top 5 Naksi King Candidates:")
            for i, candidate in enumerate(candidates[:5], 1):
                logger.info(f"   {i}. {candidate['title'][:50]}... (Score: {candidate['clickbait_score']})")

        # 5. ìƒíƒœ ì—…ë°ì´íŠ¸
        updated_count = processor.update_naksi_king_status(candidates)

        # 6. ìµœì¢… í†µê³„
        stats = NewsOperations.get_news_stats()
        logger.info(f"ğŸ“Š Final Statistics:")
        logger.info(f"   High clickbait articles: {len(high_score_news)}")
        logger.info(f"   Naksi King candidates: {len(candidates)}")
        logger.info(f"   Total news in DB: {stats}")

        log_function_end(logger, "process_naksi_king")

    except Exception as e:
        log_error(logger, e, "main process_naksi_king function")
        sys.exit(1)


if __name__ == "__main__":
    main()
