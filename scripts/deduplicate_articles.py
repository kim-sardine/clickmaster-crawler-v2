#!/usr/bin/env python3
"""
ì¤‘ë³µ ê¸°ì‚¬ í†µí•© ìŠ¤í¬ë¦½íŠ¸

ì¤‘ë³µ ê¸°ì¤€:
- title, content, journalist_name, publisherê°€ ëª¨ë‘ ë™ì¼í•˜ë©´ ì¤‘ë³µ
- clickbait_scoreê°€ ê°€ìž¥ ë†’ì€ ë‰´ìŠ¤ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ
- clickbait_scoreê°€ ë™ì¼í•˜ë©´ ê°€ìž¥ ë¨¼ì € ìƒì„±ëœ ë‰´ìŠ¤ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ
- clickbait_scoreê°€ nullì´ë©´ 0ì ìœ¼ë¡œ ê°„ì£¼
"""

import argparse
import sys
from datetime import datetime
from typing import List, Dict, Any

from src.config.settings import settings
from src.database.supabase_client import get_supabase_client
from src.utils.logging_utils import setup_logging, get_logger

setup_logging()
logger = get_logger(__name__)


def find_duplicate_groups() -> List[Dict[str, Any]]:
    """
    ì¤‘ë³µ ê¸°ì‚¬ ê·¸ë£¹ ì°¾ê¸°

    Returns:
        ì¤‘ë³µ ê·¸ë£¹ ë¦¬ìŠ¤íŠ¸ (ê° ê·¸ë£¹ì€ ì¤‘ë³µ ê¸°ì‚¬ë“¤ì˜ ì •ë³´ë¥¼ í¬í•¨)
    """
    client = get_supabase_client()

    try:
        # title, content, journalist_name, publisher ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ë³µ ì°¾ê¸°
        query = """
        SELECT 
            title,
            content,
            journalist_name,
            publisher,
            COUNT(*) as duplicate_count,
            array_agg(
                json_build_object(
                    'id', id,
                    'clickbait_score', COALESCE(clickbait_score, 0),
                    'created_at', created_at,
                    'naver_url', naver_url
                ) 
                ORDER BY COALESCE(clickbait_score, 0) DESC, created_at ASC
            ) as articles
        FROM articles
        GROUP BY title, content, journalist_name, publisher
        HAVING COUNT(*) > 1
        ORDER BY duplicate_count DESC
        """

        result = client.client.rpc("execute_sql", {"query": query}).execute()

        if result.data and result.data[0]["result"]:
            duplicate_groups = result.data[0]["result"]
            logger.info(
                f"ì¤‘ë³µ ê·¸ë£¹ ë°œê²¬: {len(duplicate_groups)}ê°œ ê·¸ë£¹, ì´ ì¤‘ë³µ ê¸°ì‚¬ ìˆ˜: {sum(g['duplicate_count'] for g in duplicate_groups)}"
            )
            return duplicate_groups
        else:
            logger.info("ì¤‘ë³µ ê¸°ì‚¬ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return []

    except Exception as e:
        logger.error(f"RPC í•¨ìˆ˜ ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.info("ëŒ€ì•ˆ ë°©ë²•ìœ¼ë¡œ ì¤‘ë³µ ê·¸ë£¹ì„ ì°¾ìŠµë‹ˆë‹¤")
        # RPC í•¨ìˆ˜ê°€ ì‹¤íŒ¨í•œ ê²½ìš° ëŒ€ì•ˆ ë°©ë²• ì‚¬ìš©
        return find_duplicate_groups_alternative()


def find_duplicate_groups_alternative() -> List[Dict[str, Any]]:
    """
    RPC í•¨ìˆ˜ê°€ ì—†ëŠ” ê²½ìš° ëŒ€ì•ˆ ë°©ë²•ìœ¼ë¡œ ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸°
    """
    client = get_supabase_client()

    try:
        # ëª¨ë“  ê¸°ì‚¬ ì¡°íšŒ
        result = client.client.table("articles").select("*").execute()

        if not result.data:
            logger.info("ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            return []

        # íŒŒì´ì¬ì—ì„œ ê·¸ë£¹í™” ì²˜ë¦¬
        content_groups = {}

        for article in result.data:
            content_key = (article["title"], article["content"], article["journalist_name"], article["publisher"])

            if content_key not in content_groups:
                content_groups[content_key] = []

            content_groups[content_key].append(
                {
                    "id": article["id"],
                    "clickbait_score": article["clickbait_score"] if article["clickbait_score"] is not None else 0,
                    "created_at": article["created_at"],
                    "naver_url": article["naver_url"],
                }
            )

        # ì¤‘ë³µ ê·¸ë£¹ë§Œ í•„í„°ë§
        duplicate_groups = []
        for (title, content, journalist_name, publisher), articles in content_groups.items():
            if len(articles) > 1:
                # clickbait_score ë‚´ë¦¼ì°¨ìˆœ, created_at ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
                articles.sort(key=lambda x: (-x["clickbait_score"], x["created_at"]))

                duplicate_groups.append(
                    {
                        "title": title,
                        "content": content,
                        "journalist_name": journalist_name,
                        "publisher": publisher,
                        "duplicate_count": len(articles),
                        "articles": articles,
                    }
                )

        # ì¤‘ë³µ ìˆ˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
        duplicate_groups.sort(key=lambda x: -x["duplicate_count"])

        if duplicate_groups:
            total_duplicates = sum(g["duplicate_count"] for g in duplicate_groups)
            logger.info(f"ì¤‘ë³µ ê·¸ë£¹ ë°œê²¬: {len(duplicate_groups)}ê°œ ê·¸ë£¹, ì´ ì¤‘ë³µ ê¸°ì‚¬ ìˆ˜: {total_duplicates}ê°œ")
        else:
            logger.info("ì¤‘ë³µ ê¸°ì‚¬ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        return duplicate_groups

    except Exception as e:
        logger.error(f"ëŒ€ì•ˆ ë°©ë²• ì¤‘ë³µ ê·¸ë£¹ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return []


def deduplicate_group(group: Dict[str, Any], dry_run: bool = False) -> int:
    """
    ì¤‘ë³µ ê·¸ë£¹ í†µí•© (ì²« ë²ˆì§¸ ê¸°ì‚¬ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ)

    Args:
        group: ì¤‘ë³µ ê·¸ë£¹ ì •ë³´
        dry_run: ì‹¤ì œ ì‚­ì œí•˜ì§€ ì•Šê³  ì‹œë®¬ë ˆì´ì…˜ë§Œ ìˆ˜í–‰

    Returns:
        ì‚­ì œëœ ê¸°ì‚¬ ìˆ˜
    """
    client = get_supabase_client()

    articles = group["articles"]
    keep_article = articles[0]  # ê°€ìž¥ ë†’ì€ ì ìˆ˜/ê°€ìž¥ ë¨¼ì € ìƒì„±ëœ ê¸°ì‚¬
    delete_articles = articles[1:]  # ë‚˜ë¨¸ì§€ ì‚­ì œí•  ê¸°ì‚¬ë“¤

    logger.info(
        f"ê·¸ë£¹ í†µí•©: '{group['title'][:50]}...' "
        f"(ìœ ì§€: {keep_article['clickbait_score']}ì , ì‚­ì œ: {len(delete_articles)}ê°œ)"
    )

    if dry_run:
        logger.info(f"[DRY RUN] {len(delete_articles)}ê°œ ê¸°ì‚¬ ì‚­ì œ ì˜ˆì •")
        return len(delete_articles)

    deleted_count = 0

    for article in delete_articles:
        try:
            result = client.client.table("articles").delete().eq("id", article["id"]).execute()

            if result.data:
                deleted_count += 1
                logger.debug(f"ê¸°ì‚¬ ì‚­ì œ ì™„ë£Œ: {article['id']}")
            else:
                logger.warning(f"ê¸°ì‚¬ ì‚­ì œ ì‹¤íŒ¨: {article['id']}")

        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ì‚­ì œ ì˜¤ë¥˜ [{article['id']}]: {e}")

    return deleted_count


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ì¤‘ë³µ ê¸°ì‚¬ í†µí•©")
    parser.add_argument("--dry-run", action="store_true", help="ì‹¤ì œ ì‚­ì œí•˜ì§€ ì•Šê³  ì‹œë®¬ë ˆì´ì…˜ë§Œ ìˆ˜í–‰")

    args = parser.parse_args()

    logger.info("ðŸ”„ ì¤‘ë³µ ê¸°ì‚¬ í†µí•© ìž‘ì—… ì‹œìž‘")
    logger.info(f"ëª¨ë“œ: {'DRY RUN' if args.dry_run else 'LIVE'}")

    try:
        # ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸°
        duplicate_groups = find_duplicate_groups()

        if not duplicate_groups:
            logger.info("ì¤‘ë³µ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            return 0

        # ëª¨ë“  ì¤‘ë³µ ê·¸ë£¹ ì²˜ë¦¬
        total_deleted = 0
        successful_groups = 0

        for i, group in enumerate(duplicate_groups, 1):
            try:
                deleted_count = deduplicate_group(group, dry_run=args.dry_run)
                total_deleted += deleted_count
                successful_groups += 1

                logger.info(
                    f"ê·¸ë£¹ {i}/{len(duplicate_groups)} ì™„ë£Œ: "
                    f"{deleted_count}ê°œ ì‚­ì œ ({'ì‹œë®¬ë ˆì´ì…˜' if args.dry_run else 'ì‹¤ì œ'})"
                )

            except Exception as e:
                logger.error(f"ê·¸ë£¹ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        # ê²°ê³¼ ìš”ì•½
        logger.info("=" * 50)
        logger.info(f"ì¤‘ë³µ ê¸°ì‚¬ í†µí•© ì™„ë£Œ")
        logger.info(f"ì²˜ë¦¬ëœ ê·¸ë£¹: {successful_groups}/{len(duplicate_groups)}")
        logger.info(f"{'ì‹œë®¬ë ˆì´ì…˜' if args.dry_run else 'ì‹¤ì œ'} ì‚­ì œëœ ê¸°ì‚¬: {total_deleted}ê°œ")

        if args.dry_run:
            logger.info("ì‹¤ì œ ì‹¤í–‰í•˜ë ¤ë©´ --dry-run ì˜µì…˜ì„ ì œê±°í•˜ì„¸ìš”")

        return total_deleted

    except Exception as e:
        logger.error(f"ì¤‘ë³µ ê¸°ì‚¬ í†µí•© ìž‘ì—… ì‹¤íŒ¨: {e}")
        return -1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(0 if exit_code >= 0 else 1)
