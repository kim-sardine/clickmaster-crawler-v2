#!/usr/bin/env python3
"""
ì¤‘ë³µ ê¸°ì‚¬ í†µí•© ìŠ¤í¬ë¦½íŠ¸

ì¤‘ë³µ ê¸°ì¤€:
- title, content, journalist_name, publisherê°€ ëª¨ë‘ ë™ì¼í•˜ë©´ ì¤‘ë³µ
- clickbait_scoreê°€ ê°€ìž¥ ë†’ì€ ë‰´ìŠ¤ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ
- clickbait_scoreê°€ ë™ì¼í•˜ë©´ ê°€ìž¥ ë¨¼ì € ìƒì„±ëœ ë‰´ìŠ¤ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ
- clickbait_scoreê°€ nullì´ë©´ 0ì ìœ¼ë¡œ ê°„ì£¼
"""

import argparse
import sys
from datetime import datetime
from typing import List, Dict, Any

from src.config.settings import settings
from src.database.supabase_client import get_supabase_client
from src.utils.logging_utils import setup_logging, get_logger

setup_logging()
logger = get_logger(__name__)


def find_duplicate_groups() -> List[Dict[str, Any]]:
    """
    ì¤‘ë³µ ê¸°ì‚¬ ê·¸ë£¹ ì°¾ê¸°

    Returns:
        ì¤‘ë³µ ê·¸ë£¹ ë¦¬ìŠ¤íŠ¸ (ê° ê·¸ë£¹ì€ ì¤‘ë³µ ê¸°ì‚¬ë“¤ì˜ ì •ë³´ë¥¼ í¬í•¨)
    """
    client = get_supabase_client()

    # title, content, journalist_name, publisher ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ë³µ ì°¾ê¸°
    query = """
    SELECT 
        title,
        content,
        journalist_name,
        publisher,
        COUNT(*) as duplicate_count,
        array_agg(
            json_build_object(
                'id', id,
                'clickbait_score', COALESCE(clickbait_score, 0),
                'created_at', created_at,
                'naver_url', naver_url
            ) 
            ORDER BY COALESCE(clickbait_score, 0) DESC, created_at ASC
        ) as articles
    FROM articles
    GROUP BY title, content, journalist_name, publisher
    HAVING COUNT(*) > 1
    ORDER BY duplicate_count DESC
    """

    result = client.client.rpc("execute_sql", {"query": query}).execute()

    if result.data and result.data[0]["result"]:
        duplicate_groups = result.data[0]["result"]
        logger.info(
            f"ì¤‘ë³µ ê·¸ë£¹ ë°œê²¬: {len(duplicate_groups)}ê°œ ê·¸ë£¹, ì´ ì¤‘ë³µ ê¸°ì‚¬ ìˆ˜: {sum(g['duplicate_count'] for g in duplicate_groups)}"
        )
        return duplicate_groups
    else:
        logger.info("ì¤‘ë³µ ê¸°ì‚¬ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return []


def deduplicate_group(group: Dict[str, Any], dry_run: bool = False) -> int:
    """
    ì¤‘ë³µ ê·¸ë£¹ í†µí•© (ì²« ë²ˆì§¸ ê¸°ì‚¬ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ)

    Args:
        group: ì¤‘ë³µ ê·¸ë£¹ ì •ë³´
        dry_run: ì‹¤ì œ ì‚­ì œí•˜ì§€ ì•Šê³  ì‹œë®¬ë ˆì´ì…˜ë§Œ ìˆ˜í–‰

    Returns:
        ì‚­ì œëœ ê¸°ì‚¬ ìˆ˜
    """
    client = get_supabase_client()

    articles = group["articles"]
    keep_article = articles[0]  # ê°€ìž¥ ë†’ì€ ì ìˆ˜/ê°€ìž¥ ë¨¼ì € ìƒì„±ëœ ê¸°ì‚¬
    delete_articles = articles[1:]  # ë‚˜ë¨¸ì§€ ì‚­ì œí•  ê¸°ì‚¬ë“¤

    logger.info(
        f"ê·¸ë£¹ í†µí•©: '{group['title'][:50]}...' "
        f"(ìœ ì§€: {keep_article['clickbait_score']}ì , ì‚­ì œ: {len(delete_articles)}ê°œ)"
    )

    if dry_run:
        logger.info(f"[DRY RUN] {len(delete_articles)}ê°œ ê¸°ì‚¬ ì‚­ì œ ì˜ˆì •")
        return len(delete_articles)

    deleted_count = 0

    for article in delete_articles:
        try:
            result = client.client.table("articles").delete().eq("id", article["id"]).execute()

            if result.data:
                deleted_count += 1
                logger.debug(f"ê¸°ì‚¬ ì‚­ì œ ì™„ë£Œ: {article['id']}")
            else:
                logger.warning(f"ê¸°ì‚¬ ì‚­ì œ ì‹¤íŒ¨: {article['id']}")

        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ì‚­ì œ ì˜¤ë¥˜ [{article['id']}]: {e}")

    return deleted_count


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ì¤‘ë³µ ê¸°ì‚¬ í†µí•©")
    parser.add_argument("--dry-run", action="store_true", help="ì‹¤ì œ ì‚­ì œí•˜ì§€ ì•Šê³  ì‹œë®¬ë ˆì´ì…˜ë§Œ ìˆ˜í–‰")

    args = parser.parse_args()

    logger.info("ðŸ”„ ì¤‘ë³µ ê¸°ì‚¬ í†µí•© ìž‘ì—… ì‹œìž‘")
    logger.info(f"ëª¨ë“œ: {'DRY RUN' if args.dry_run else 'LIVE'}")

    try:
        # ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸°
        duplicate_groups = find_duplicate_groups()

        if not duplicate_groups:
            logger.info("ì¤‘ë³µ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            return 0

        # ëª¨ë“  ì¤‘ë³µ ê·¸ë£¹ ì²˜ë¦¬
        total_deleted = 0
        successful_groups = 0

        for i, group in enumerate(duplicate_groups, 1):
            try:
                deleted_count = deduplicate_group(group, dry_run=args.dry_run)
                total_deleted += deleted_count
                successful_groups += 1

                logger.info(
                    f"ê·¸ë£¹ {i}/{len(duplicate_groups)} ì™„ë£Œ: "
                    f"{deleted_count}ê°œ ì‚­ì œ ({'ì‹œë®¬ë ˆì´ì…˜' if args.dry_run else 'ì‹¤ì œ'})"
                )

            except Exception as e:
                logger.error(f"ê·¸ë£¹ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        # ê²°ê³¼ ìš”ì•½
        logger.info("=" * 50)
        logger.info(f"ì¤‘ë³µ ê¸°ì‚¬ í†µí•© ì™„ë£Œ")
        logger.info(f"ì²˜ë¦¬ëœ ê·¸ë£¹: {successful_groups}/{len(duplicate_groups)}")
        logger.info(f"{'ì‹œë®¬ë ˆì´ì…˜' if args.dry_run else 'ì‹¤ì œ'} ì‚­ì œëœ ê¸°ì‚¬: {total_deleted}ê°œ")

        if args.dry_run:
            logger.info("ì‹¤ì œ ì‹¤í–‰í•˜ë ¤ë©´ --dry-run ì˜µì…˜ì„ ì œê±°í•˜ì„¸ìš”")

        return total_deleted

    except Exception as e:
        logger.error(f"ì¤‘ë³µ ê¸°ì‚¬ í†µí•© ìž‘ì—… ì‹¤íŒ¨: {e}")
        return -1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(0 if exit_code >= 0 else 1)
