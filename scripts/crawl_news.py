#!/usr/bin/env python3
"""
ë‰´ìŠ¤ í¬ë¡¤ë§ ë° Supabase ì €ì¥ ìŠ¤í¬ë¦½íŠ¸
ë§¤ì¼ ì˜¤ì „ 6ì‹œì— ì‹¤í–‰ë˜ì–´ ì „ë‚  ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ì—¬ Supabaseì— ì €ì¥
"""

import sys
import os
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import logging
from typing import List, Optional
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.config.settings import Settings
from src.config.keywords import get_keywords_by_category
from src.models.base import News
from src.database.operations import NewsOperations
from src.utils.logging_utils import setup_logger, log_function_start, log_function_end, log_error, log_progress
from src.utils.date_utils import get_yesterday_date_range, parse_naver_date, is_within_date_range
from src.utils.text_utils import clean_html, extract_main_content, is_valid_title, is_valid_content

logger = setup_logger(__name__)


class NaverNewsCrawler:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(Settings.get_headers())
        self.collected_urls = set()

    def search_news_by_keyword(self, keyword: str, start_date: str, end_date: str) -> List[dict]:
        """í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰"""
        try:
            url = "https://openapi.naver.com/v1/search/news.json"

            all_news = []
            start = 1

            while len(all_news) < Settings.MAX_NEWS_PER_KEYWORD:
                params = {"query": keyword, "display": Settings.NAVER_DISPLAY_COUNT, "start": start, "sort": "date"}

                response = self.session.get(url, params=params)
                response.raise_for_status()

                data = response.json()
                items = data.get("items", [])

                if not items:
                    break

                # ë‚ ì§œ í•„í„°ë§
                filtered_items = []
                for item in items:
                    pub_date = parse_naver_date(item["pubDate"])
                    if is_within_date_range(pub_date, start_date, end_date):
                        # URL ì¤‘ë³µ ì œê±°
                        if item["link"] not in self.collected_urls:
                            filtered_items.append(item)
                            self.collected_urls.add(item["link"])

                all_news.extend(filtered_items)

                # ë‹¤ìŒ í˜ì´ì§€
                start += Settings.NAVER_DISPLAY_COUNT

                # API í˜¸ì¶œ ì œí•œ ê³ ë ¤
                time.sleep(Settings.REQUEST_DELAY)

                # ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                if len(items) < Settings.NAVER_DISPLAY_COUNT:
                    break

            logger.info(f"ğŸ” Found {len(all_news)} news articles for keyword: {keyword}")
            return all_news[: Settings.MAX_NEWS_PER_KEYWORD]

        except Exception as e:
            log_error(logger, e, f"searching news for keyword: {keyword}")
            return []

    def crawl_article_content(self, url: str) -> Optional[dict]:
        """ê°œë³„ ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # ë„¤ì´ë²„ ë‰´ìŠ¤ êµ¬ì¡°ì— ë”°ë¥¸ ë‚´ìš© ì¶”ì¶œ
            content_selectors = [
                "#dic_area",  # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸
                ".news_end",  # ì¼ë¶€ ì–¸ë¡ ì‚¬
                "#articleBodyContents",  # ì¼ë¶€ ì–¸ë¡ ì‚¬
                ".article_body",  # ì¼ë¶€ ì–¸ë¡ ì‚¬
                'div[itemprop="articleBody"]',  # ì¼ë°˜ì ì¸ êµ¬ì¡°
            ]

            content = ""
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    content = element.get_text(strip=True)
                    break

            if not content:
                # ì¼ë°˜ì ì¸ p íƒœê·¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                paragraphs = soup.find_all("p")
                content = " ".join([p.get_text(strip=True) for p in paragraphs])

            # ê¸°ìëª… ì¶”ì¶œ
            author = None
            author_selectors = [".byline", ".reporter", ".author", "[data-reporter]"]

            for selector in author_selectors:
                element = soup.select_one(selector)
                if element:
                    author = element.get_text(strip=True)
                    break

            # ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ
            source = ""
            source_selectors = [".press", ".source", ".media", "[data-press]"]

            for selector in source_selectors:
                element = soup.select_one(selector)
                if element:
                    source = element.get_text(strip=True)
                    break

            return {"content": content, "author": author, "source": source}

        except Exception as e:
            log_error(logger, e, f"crawling article: {url}")
            return None

    def process_news_item(self, item: dict) -> Optional[News]:
        """ë‰´ìŠ¤ ì•„ì´í…œ ì²˜ë¦¬"""
        try:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = clean_html(item["title"])
            url = item["link"]
            pub_date = parse_naver_date(item["pubDate"]).strftime("%Y-%m-%d")

            # ì œëª© ìœ íš¨ì„± ê²€ì‚¬
            if not is_valid_title(title):
                logger.debug(f"Invalid title: {title}")
                return None

            # URL ì¤‘ë³µ í™•ì¸
            if NewsOperations.check_duplicate_url(url):
                logger.debug(f"Duplicate URL: {url}")
                return None

            # ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§
            article_data = self.crawl_article_content(url)
            if not article_data:
                return None

            content = extract_main_content(article_data["content"])

            # ë³¸ë¬¸ ìœ íš¨ì„± ê²€ì‚¬
            if not is_valid_content(content):
                logger.debug(f"Invalid content for URL: {url}")
                return None

            return News(
                title=title,
                content=content,
                url=url,
                published_date=pub_date,
                source=article_data["source"] or "Unknown",
                author=article_data["author"],
            )

        except Exception as e:
            log_error(logger, e, f"processing news item: {item.get('title', 'Unknown')}")
            return None


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        log_function_start(logger, "crawl_news")

        # í™˜ê²½ë³€ìˆ˜ ê²€ì¦
        Settings.validate_required_env_vars()

        # ì–´ì œ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        start_date, end_date = get_yesterday_date_range()
        logger.info(f"ğŸ“… Crawling news for date range: {start_date} - {end_date}")

        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = NaverNewsCrawler()

        # í‚¤ì›Œë“œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        keywords = get_keywords_by_category()
        logger.info(f"ğŸ” Starting crawl with {len(keywords)} keywords")

        all_news = []
        total_collected = 0

        # í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
        for i, keyword in enumerate(tqdm(keywords, desc="Keywords")):
            try:
                # ë„¤ì´ë²„ APIë¡œ ë‰´ìŠ¤ ê²€ìƒ‰
                search_results = crawler.search_news_by_keyword(keyword, start_date, end_date)

                if not search_results:
                    continue

                # ê° ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
                keyword_news = []
                for item in tqdm(search_results, desc=f"Processing {keyword}", leave=False):
                    news = crawler.process_news_item(item)
                    if news:
                        keyword_news.append(news)

                    # ìš”ì²­ ì œí•œ ê³ ë ¤
                    time.sleep(Settings.REQUEST_DELAY)

                all_news.extend(keyword_news)
                total_collected += len(keyword_news)

                log_progress(logger, i + 1, len(keywords), f"Keywords processed")
                logger.info(f"ğŸ“Š Keyword '{keyword}': {len(keyword_news)} valid articles")

            except Exception as e:
                log_error(logger, e, f"processing keyword: {keyword}")
                continue

        logger.info(f"ğŸ“ˆ Total news collected: {total_collected}")

        # Supabaseì— ì €ì¥
        if all_news:
            logger.info("ğŸ’¾ Saving news to Supabase...")
            saved_count = NewsOperations.insert_news_batch(all_news)
            logger.info(f"âœ… Successfully saved {saved_count} news articles to Supabase")
        else:
            logger.warning("âš ï¸ No news articles to save")

        # ìµœì¢… í†µê³„
        stats = NewsOperations.get_news_stats()
        logger.info(f"ğŸ“Š Final stats: {stats}")

        log_function_end(logger, "crawl_news")

    except Exception as e:
        log_error(logger, e, "main crawl_news function")
        sys.exit(1)


if __name__ == "__main__":
    main()
