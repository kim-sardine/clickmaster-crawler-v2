import requests
import time
from bs4 import BeautifulSoup
from typing import List, Optional, Dict, Any
import logging

from ..config.settings import Settings
from ..models.base import News
from ..utils.logging_utils import log_api_call, log_error
from ..utils.date_utils import parse_naver_date, is_within_date_range
from ..utils.text_utils import clean_html, extract_main_content, is_valid_title, is_valid_content

logger = logging.getLogger(__name__)


class NaverNewsCrawler:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(Settings.get_headers())
        self.collected_urls = set()

    def search_news_by_keyword(self, keyword: str, start_date: str, end_date: str) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰"""
        try:
            url = "https://openapi.naver.com/v1/search/news.json"
            log_api_call(logger, "Naver Search", f"/search/news.json?query={keyword}")

            all_news = []
            start = 1

            while len(all_news) < Settings.MAX_NEWS_PER_KEYWORD:
                params = {"query": keyword, "display": Settings.NAVER_DISPLAY_COUNT, "start": start, "sort": "date"}

                response = self.session.get(url, params=params)
                response.raise_for_status()

                log_api_call(
                    logger, "Naver Search", f"page {start // Settings.NAVER_DISPLAY_COUNT + 1}", response.status_code
                )

                data = response.json()
                items = data.get("items", [])

                if not items:
                    break

                # ë‚ ì§œ í•„í„°ë§ ë° ì¤‘ë³µ ì œê±°
                filtered_items = []
                for item in items:
                    pub_date = parse_naver_date(item["pubDate"])
                    if is_within_date_range(pub_date, start_date, end_date):
                        if item["link"] not in self.collected_urls:
                            filtered_items.append(item)
                            self.collected_urls.add(item["link"])

                all_news.extend(filtered_items)

                # ë‹¤ìŒ í˜ì´ì§€
                start += Settings.NAVER_DISPLAY_COUNT

                # API í˜¸ì¶œ ì œí•œ ê³ ë ¤
                time.sleep(Settings.REQUEST_DELAY)

                # ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                if len(items) < Settings.NAVER_DISPLAY_COUNT:
                    break

            logger.info(f"ğŸ” Found {len(all_news)} news articles for keyword: {keyword}")
            return all_news[: Settings.MAX_NEWS_PER_KEYWORD]

        except Exception as e:
            log_error(logger, e, f"searching news for keyword: {keyword}")
            return []

    def extract_article_content(self, url: str) -> Optional[Dict[str, str]]:
        """ê°œë³„ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # ë„¤ì´ë²„ ë‰´ìŠ¤ êµ¬ì¡°ì— ë”°ë¥¸ ë‚´ìš© ì¶”ì¶œ
            content_selectors = [
                "#dic_area",  # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸
                ".news_end",  # ì¼ë¶€ ì–¸ë¡ ì‚¬
                "#articleBodyContents",  # ì¼ë¶€ ì–¸ë¡ ì‚¬
                ".article_body",  # ì¼ë¶€ ì–¸ë¡ ì‚¬
                'div[itemprop="articleBody"]',  # ì¼ë°˜ì ì¸ êµ¬ì¡°
                ".article-body",  # ì¶”ê°€ íŒ¨í„´
                ".news-article-body",  # ì¶”ê°€ íŒ¨í„´
            ]

            content = ""
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    content = element.get_text(strip=True)
                    break

            if not content:
                # ì¼ë°˜ì ì¸ p íƒœê·¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                paragraphs = soup.find_all("p")
                content = " ".join([p.get_text(strip=True) for p in paragraphs])

            # ê¸°ìëª… ì¶”ì¶œ
            author = self._extract_author(soup)

            # ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ
            source = self._extract_source(soup)

            return {"content": content, "author": author, "source": source}

        except Exception as e:
            log_error(logger, e, f"extracting article content: {url}")
            return None

    def _extract_author(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ìëª… ì¶”ì¶œ"""
        author_selectors = [
            ".byline",
            ".reporter",
            ".author",
            "[data-reporter]",
            ".journalist",
            ".writer",
            ".article-reporter",
        ]

        for selector in author_selectors:
            element = soup.select_one(selector)
            if element:
                author_text = element.get_text(strip=True)
                # "ê¸°ì" ë“±ì˜ ë‹¨ì–´ ì œê±°
                author_text = author_text.replace("ê¸°ì", "").replace("íŠ¹íŒŒì›", "").strip()
                if author_text:
                    return author_text

        # í…ìŠ¤íŠ¸ì—ì„œ "XXX ê¸°ì" íŒ¨í„´ ì°¾ê¸°
        import re

        text = soup.get_text()
        author_match = re.search(r"([ê°€-í£]{2,4})\s*ê¸°ì", text)
        if author_match:
            return author_match.group(1)

        return None

    def _extract_source(self, soup: BeautifulSoup) -> str:
        """ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ"""
        source_selectors = [
            ".press",
            ".source",
            ".media",
            "[data-press]",
            ".news-source",
            ".article-source",
            ".media-name",
        ]

        for selector in source_selectors:
            element = soup.select_one(selector)
            if element:
                source_text = element.get_text(strip=True)
                if source_text:
                    return source_text

        # meta íƒœê·¸ì—ì„œ ì¶”ì¶œ ì‹œë„
        meta_selectors = ['meta[property="og:site_name"]', 'meta[name="author"]', 'meta[property="article:publisher"]']

        for selector in meta_selectors:
            element = soup.select_one(selector)
            if element and element.get("content"):
                return element.get("content")

        return "Unknown"

    def process_news_item(self, item: Dict[str, Any]) -> Optional[News]:
        """ë‰´ìŠ¤ ì•„ì´í…œì„ News ê°ì²´ë¡œ ë³€í™˜"""
        try:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = clean_html(item["title"])
            url = item["link"]
            pub_date = parse_naver_date(item["pubDate"]).strftime("%Y-%m-%d")

            # ì œëª© ìœ íš¨ì„± ê²€ì‚¬
            if not is_valid_title(title):
                logger.debug(f"Invalid title: {title}")
                return None

            # ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§
            article_data = self.extract_article_content(url)
            if not article_data:
                return None

            content = extract_main_content(article_data["content"])

            # ë³¸ë¬¸ ìœ íš¨ì„± ê²€ì‚¬
            if not is_valid_content(content):
                logger.debug(f"Invalid content for URL: {url}")
                return None

            return News(
                title=title,
                content=content,
                url=url,
                published_date=pub_date,
                source=article_data["source"] or "Unknown",
                author=article_data["author"],
            )

        except Exception as e:
            log_error(logger, e, f"processing news item: {item.get('title', 'Unknown')}")
            return None

    def get_crawl_stats(self) -> Dict[str, int]:
        """í¬ë¡¤ë§ í†µê³„ ë°˜í™˜"""
        return {"collected_urls": len(self.collected_urls)}
