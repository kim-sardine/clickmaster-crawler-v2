"""
OpenAI Batch API ì²˜ë¦¬ ëª¨ë“ˆ
"""

import json
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional

from .openai_client import OpenAIClient
from .prompt_generator import PromptGenerator
from .bulk_updater import BulkUpdater
from src.utils.logging_utils import get_logger

logger = get_logger(__name__)


class BatchProcessor:
    """ë°°ì¹˜ ì²˜ë¦¬ í•µì‹¬ ë¡œì§"""

    def __init__(
        self, supabase, openai_client: OpenAIClient, prompt_generator: PromptGenerator, bulk_updater: BulkUpdater
    ):
        """
        Args:
            supabase: Supabase í´ë¼ì´ì–¸íŠ¸
            openai_client: OpenAI í´ë¼ì´ì–¸íŠ¸
            prompt_generator: í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°
            bulk_updater: ë²Œí¬ ì—…ë°ì´í„°
        """
        self.supabase = supabase
        self.openai_client = openai_client
        self.prompt_generator = prompt_generator
        self.bulk_updater = bulk_updater

    def check_active_batch(self) -> Optional[Dict[str, Any]]:
        """
        í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ë°°ì¹˜ í™•ì¸ (ìƒì„± ì‹œê°„ ê¸°ì¤€ ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ)

        Returns:
            í™œì„± ë°°ì¹˜ ì •ë³´ ë˜ëŠ” None
        """
        logger.info("Checking for active batches")

        try:
            response = (
                self.supabase.client.table("batch")
                .select("*")
                .eq("status", "in_progress")
                .order("created_at", desc=False)  # ê°€ì¥ ì˜¤ë˜ëœ ê²ƒë¶€í„°
                .execute()
            )

            if response.data:
                # ì—¬ëŸ¬ ë°°ì¹˜ê°€ ìˆëŠ” ê²½ìš° ê²½ê³  ë¡œê·¸
                if len(response.data) > 1:
                    logger.warning(f"Found {len(response.data)} active batches! Processing oldest first.")
                    for i, batch in enumerate(response.data):
                        logger.warning(
                            f"  Batch {i + 1}: {batch['batch_id']} (created: {batch.get('created_at', 'unknown')})"
                        )

                active_batch = response.data[0]
                logger.info(f"Processing active batch: {active_batch['batch_id']}")
                return active_batch
            else:
                logger.info("No active batch found")

                # ğŸ” ê³ ì•„ ë°°ì¹˜ ê°ì§€ (OpenAIì—ëŠ” ìˆì§€ë§Œ DBì—ëŠ” ì—†ëŠ” ë°°ì¹˜)
                self._detect_and_recover_orphan_batches()

                return None

        except Exception as e:
            logger.error(f"Failed to check active batch: {e}")
            return None

    def _detect_and_recover_orphan_batches(self):
        """
        ê³ ì•„ ë°°ì¹˜ ê°ì§€ ë° ë³µêµ¬ ì‹œë„
        """
        logger.info("ğŸ” Checking for orphan batches (OpenAI batches not recorded in database)")

        try:
            # TODO: OpenAI APIì—ì„œ ë‚´ ê³„ì •ì˜ ëª¨ë“  ë°°ì¹˜ ëª©ë¡ì„ ì¡°íšŒí•˜ëŠ” ê¸°ëŠ¥ì´ ìˆë‹¤ë©´ êµ¬í˜„
            # í˜„ì¬ëŠ” ë¡œê¹…ë§Œ ìˆ˜í–‰
            logger.info("Orphan batch detection: Feature planned for future implementation")
            logger.info("If you suspect orphan batches exist, check OpenAI dashboard manually")

        except Exception as e:
            logger.warning(f"Failed to detect orphan batches: {e}")

    def recover_orphan_batch(self, batch_id: str, article_count: int) -> bool:
        """
        ê³ ì•„ ë°°ì¹˜ ë³µêµ¬ (OpenAIì—ëŠ” ìˆì§€ë§Œ DBì—ëŠ” ì—†ëŠ” ë°°ì¹˜ë¥¼ DBì— ë“±ë¡)

        Args:
            batch_id: OpenAI ë°°ì¹˜ ID
            article_count: ì˜ˆìƒ Article ìˆ˜

        Returns:
            ë³µêµ¬ ì„±ê³µ ì—¬ë¶€
        """
        logger.info(f"Attempting to recover orphan batch: {batch_id}")

        try:
            # 1. OpenAIì—ì„œ ë°°ì¹˜ ìƒíƒœ í™•ì¸
            openai_status = self.check_batch_completion(batch_id)

            if not openai_status:
                logger.error(f"Cannot recover orphan batch: OpenAI batch {batch_id} not found")
                return False

            logger.info(f"Found OpenAI batch {batch_id} with status: {openai_status}")

            # 2. DBì— ë°°ì¹˜ ì •ë³´ ë³µêµ¬
            batch_data = {
                "batch_id": batch_id,
                "status": "in_progress" if openai_status == "in_progress" else openai_status,
                "article_count": article_count,
                "created_at": datetime.now().isoformat(),
                "recovered": True,  # ë³µêµ¬ëœ ë°°ì¹˜ì„ì„ í‘œì‹œ
            }

            response = self.supabase.client.table("batch").insert(batch_data).execute()

            if response.data:
                logger.info(f"âœ… Successfully recovered orphan batch: {batch_id}")
                logger.info(f"  Status: {openai_status}")
                logger.info(f"  Article count: {article_count}")
                return True
            else:
                logger.error(f"Failed to recover orphan batch: No data returned")
                return False

        except Exception as e:
            logger.error(f"Failed to recover orphan batch {batch_id}: {e}")
            return False

    def get_all_active_batches(self) -> List[Dict[str, Any]]:
        """
        ëª¨ë“  í™œì„± ë°°ì¹˜ ì¡°íšŒ (ëª¨ë‹ˆí„°ë§/ë””ë²„ê¹…ìš©)

        Returns:
            í™œì„± ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("Retrieving all active batches")

        try:
            response = (
                self.supabase.client.table("batch")
                .select("*")
                .eq("status", "in_progress")
                .order("created_at", desc=False)
                .execute()
            )

            batches = response.data
            logger.info(f"Found {len(batches)} active batches")

            for i, batch in enumerate(batches):
                logger.info(f"  Batch {i + 1}: {batch['batch_id']} (created: {batch.get('created_at', 'unknown')})")

            return batches

        except Exception as e:
            logger.error(f"Failed to retrieve active batches: {e}")
            return []

    def get_pending_articles(self, limit: int = 800) -> List[Dict[str, Any]]:
        """
        ë¯¸ì²˜ë¦¬ Article ë°ì´í„° ì¡°íšŒ

        Args:
            limit: ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜

        Returns:
            ë¯¸ì²˜ë¦¬ Article ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"Retrieving pending articles (limit: {limit})")

        try:
            response = (
                self.supabase.client.table("articles")
                .select("*")
                .is_("clickbait_score", "null")
                .order("created_at", desc=False)
                .limit(limit)
                .execute()
            )

            articles = response.data
            logger.info(f"Found {len(articles)} pending articles")
            return articles

        except Exception as e:
            logger.error(f"Failed to retrieve pending articles: {e}")
            return []

    def create_batch_request(self, articles: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """
        ë°°ì¹˜ ìš”ì²­ ìƒì„± (ë™ì‹œì„± ì œì–´ í¬í•¨)

        Args:
            articles: Article ë°ì´í„° ë¦¬ìŠ¤íŠ¸

        Returns:
            ìƒì„±ëœ ë°°ì¹˜ ì •ë³´ ë˜ëŠ” None
        """
        if not articles:
            logger.warning("No articles to process")
            return None

        logger.info(f"Creating batch request for {len(articles)} articles")

        # ğŸ”’ ë°°ì¹˜ ìƒì„± ì „ ì‚¬ì „ ë™ì‹œì„± ì²´í¬ (Race Condition ë°©ì§€)
        if not self._pre_check_batch_creation():
            logger.warning("Pre-check failed: Another batch is already in progress")
            return None

        try:
            # ë°°ì¹˜ ìš”ì²­ ìƒì„±
            batch_requests = self.prompt_generator.generate_batch_requests(articles)

            if not batch_requests:
                logger.warning("No valid batch requests generated")
                return None

            # OpenAI ë°°ì¹˜ ìƒì„±
            batch = self.openai_client.create_batch(batch_requests)

            batch_info = {
                "id": batch["id"] if isinstance(batch, dict) else batch.id,
                "status": batch["status"] if isinstance(batch, dict) else batch.status,
                "created_at": datetime.now().isoformat(),
            }

            batch_id = batch["id"] if isinstance(batch, dict) else batch.id
            logger.info(f"Batch created successfully: {batch_id}")
            return batch_info

        except Exception as e:
            logger.error(f"Failed to create batch request: {e}")
            return None

    def _pre_check_batch_creation(self) -> bool:
        """
        ë°°ì¹˜ ìƒì„± ì „ ì‚¬ì „ ì²´í¬ (ë™ì‹œì„± ì œì–´)

        Returns:
            ë°°ì¹˜ ìƒì„± ê°€ëŠ¥ ì—¬ë¶€
        """
        logger.info("Performing pre-check for batch creation")

        try:
            # í˜„ì¬ í™œì„± ë°°ì¹˜ í™•ì¸
            active_check = (
                self.supabase.client.table("batch")
                .select("id, batch_id, status, created_at")
                .eq("status", "in_progress")
                .execute()
            )

            if active_check.data:
                active_batch = active_check.data[0]
                logger.warning(f"Pre-check failed: Active batch exists - {active_batch['batch_id']}")
                logger.warning(f"  Created at: {active_batch.get('created_at', 'unknown')}")
                logger.warning(f"  Status: {active_batch['status']}")
                return False

            logger.info("Pre-check passed: No active batches found")
            return True

        except Exception as e:
            logger.error(f"Pre-check failed due to error: {e}")
            return False

    def process_batch_results(self, batch_id: str) -> bool:
        """
        ë°°ì¹˜ ê²°ê³¼ ì²˜ë¦¬ (ë©±ë“±ì„± ìµœì í™” í¬í•¨)

        Args:
            batch_id: ë°°ì¹˜ ID

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        logger.info(f"Processing batch results: {batch_id} (with idempotency optimization)")

        try:
            # 1ë‹¨ê³„: ë°°ì¹˜ ìƒíƒœ í™•ì¸ - ì´ë¯¸ ì™„ë£Œëœ ê²½ìš° ìŠ¤í‚µ
            existing_batch = self._get_batch_info(batch_id)
            if existing_batch and existing_batch.get("status") == "completed":
                logger.info(f"Batch {batch_id} is already completed - operation is idempotent")
                return True

            # 2ë‹¨ê³„: ë°°ì¹˜ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (ìºì‹± ê³ ë ¤)
            logger.info("Downloading batch results from OpenAI")
            results = self._get_cached_or_download_results(batch_id)

            if not results:
                logger.error("No results found in batch - batch may be incomplete or corrupted")
                return False

            logger.info(f"Downloaded {len(results)} results, starting parsing...")

            # 3ë‹¨ê³„: ê²°ê³¼ íŒŒì‹± (íŒŒì‹± ì—ëŸ¬ ì‹œ exception ë°œìƒ)
            updates = self._parse_batch_results(results)

            if not updates:
                logger.error("No valid updates parsed from results - this should not happen in strict mode")
                return False

            logger.info(f"Successfully parsed {len(updates)} updates, starting bulk update...")

            # 4ë‹¨ê³„: ë©±ë“±ì„±ì„ ê³ ë ¤í•œ ë²Œí¬ ì—…ë°ì´íŠ¸ ìˆ˜í–‰
            success = self.bulk_updater.bulk_update_articles(updates)

            if success:
                logger.info(f"Successfully processed batch {batch_id} with idempotency guarantees")
                return True
            else:
                logger.error("Bulk update failed - database operation error")
                return False

        except Exception as e:
            error_message = str(e)

            # íŒŒì‹± ì—ëŸ¬ì™€ ë‹¤ë¥¸ ì—ëŸ¬ë¥¼ êµ¬ë¶„
            if "Batch parsing failed:" in error_message:
                logger.error(f"Batch processing failed due to parsing errors: {e}")
                logger.error("This indicates issues with OpenAI response format or content validation")
            elif "No results found" in error_message:
                logger.error(f"Batch processing failed - no results available: {e}")
                logger.error("This may indicate OpenAI batch completion issues")
            else:
                logger.error(f"Batch processing failed due to unexpected error: {e}")
                logger.error("This may indicate system/network issues")

            return False

    def _parse_batch_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ë°°ì¹˜ ê²°ê³¼ íŒŒì‹± (íŒŒì‹± ì—ëŸ¬ ì‹œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨)

        Args:
            results: OpenAI ë°°ì¹˜ ê²°ê³¼

        Returns:
            íŒŒì‹±ëœ ì—…ë°ì´íŠ¸ ë°ì´í„°

        Raises:
            Exception: íŒŒì‹± ì—ëŸ¬ ë°œìƒ ì‹œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨
        """
        logger.info(f"Parsing {len(results)} batch results (strict mode: any parsing error will fail entire process)")

        updates = []
        parsing_errors = []

        for i, result in enumerate(results, 1):
            try:
                # Article ID ì¶”ì¶œ
                custom_id = result.get("custom_id", "")
                if not custom_id.startswith("article_"):
                    error_msg = f"Result {i}: Invalid custom_id format: {custom_id}"
                    logger.error(error_msg)
                    parsing_errors.append(error_msg)
                    continue

                article_id = custom_id.replace("article_", "")

                # OpenAI ì‘ë‹µ ì¶”ì¶œ
                response_body = result.get("response", {}).get("body", {})
                choices = response_body.get("choices", [])

                if not choices:
                    error_msg = f"Result {i} (article {article_id}): No choices in response"
                    logger.error(error_msg)
                    parsing_errors.append(error_msg)
                    continue

                message_content = choices[0].get("message", {}).get("content", "")

                if not message_content:
                    error_msg = f"Result {i} (article {article_id}): Empty message content"
                    logger.error(error_msg)
                    parsing_errors.append(error_msg)
                    continue

                # PromptGeneratorì˜ validate_clickbait_response ì‚¬ìš©
                validated_data = self.prompt_generator.validate_clickbait_response(message_content)

                if validated_data:
                    # UUIDëŠ” ë¬¸ìì—´ í˜•íƒœë¡œ ì‚¬ìš©
                    update = {
                        "id": article_id,
                        "clickbait_score": validated_data["clickbait_score"],
                        "clickbait_explanation": validated_data["clickbait_explanation"],
                    }
                    updates.append(update)
                    logger.debug(f"Result {i} (article {article_id}): Successfully parsed")
                else:
                    error_msg = f"Result {i} (article {article_id}): Invalid response data - validation failed"
                    logger.error(error_msg)
                    parsing_errors.append(error_msg)

            except Exception as e:
                error_msg = f"Result {i}: Critical parsing error: {e}"
                logger.error(error_msg)
                parsing_errors.append(error_msg)

        # íŒŒì‹± ì—ëŸ¬ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨
        if parsing_errors:
            total_errors = len(parsing_errors)
            error_summary = f"Batch parsing failed: {total_errors} errors out of {len(results)} results"
            logger.error(error_summary)
            logger.error("Parsing errors details:")
            for error in parsing_errors:
                logger.error(f"  - {error}")

            # ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨
            raise Exception(f"{error_summary}. First error: {parsing_errors[0]}")

        logger.info(f"Successfully parsed all {len(updates)} results without errors")
        return updates

    def save_batch_info_to_database(self, batch_info: Dict[str, Any], article_count: int) -> Optional[Dict[str, Any]]:
        """
        ë°°ì¹˜ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ê°•í™”ëœ ì›ìì  ì²˜ë¦¬ë¡œ ì¤‘ë³µ ë°©ì§€)

        Args:
            batch_info: ë°°ì¹˜ ì •ë³´
            article_count: ì²˜ë¦¬í•  Article ìˆ˜

        Returns:
            ì €ì¥ëœ ë°ì´í„° ë˜ëŠ” None
        """
        batch_id = batch_info["id"]
        logger.info(f"ğŸ”’ Saving batch info to database with enhanced concurrency control: {batch_id}")

        try:
            # 1ë‹¨ê³„: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë°°ì¹˜ì¸ì§€ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
            logger.info("Step 1: Checking for existing batch")
            existing_check = (
                self.supabase.client.table("batch").select("id, batch_id, status").eq("batch_id", batch_id).execute()
            )

            if existing_check.data:
                existing_batch = existing_check.data[0]
                logger.warning(f"âŒ Batch already exists in database: {batch_id}")
                logger.warning(f"  Existing batch status: {existing_batch['status']}")
                logger.warning(f"  Database ID: {existing_batch['id']}")
                return existing_batch

            # 2ë‹¨ê³„: ë‹¤ë¥¸ in_progress ë°°ì¹˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ë™ì‹œì„± ì œì–´)
            logger.info("Step 2: Checking for concurrent in_progress batches")
            active_check = (
                self.supabase.client.table("batch")
                .select("id, batch_id, status, created_at")
                .eq("status", "in_progress")
                .execute()
            )

            if active_check.data:
                concurrent_batch = active_check.data[0]
                logger.warning("âŒ Concurrent batch creation prevented!")
                logger.warning(f"  Another batch is already in progress: {concurrent_batch['batch_id']}")
                logger.warning(f"  Concurrent batch created: {concurrent_batch.get('created_at', 'unknown')}")
                logger.warning(f"  Current batch to save: {batch_id}")
                logger.warning("ğŸ›¡ï¸ Skipping batch creation to prevent concurrent processing")

                # OpenAI ë°°ì¹˜ëŠ” ìƒì„±ë˜ì—ˆì§€ë§Œ DB ì €ì¥ ë¶ˆê°€ - ì¤‘ìš”í•œ ìƒí™©
                logger.error(f"âš ï¸ CRITICAL: OpenAI batch {batch_id} was created but cannot be saved due to concurrency")
                logger.error("This may require manual intervention to clean up the orphan batch")

                return None

            # 3ë‹¨ê³„: ë°°ì¹˜ ì •ë³´ ì‚½ì… (ì›ìì  ì²˜ë¦¬)
            logger.info("Step 3: Inserting new batch into database")
            batch_data = {
                "batch_id": batch_id,
                "status": "in_progress",
                "article_count": article_count,
                "created_at": batch_info.get("created_at", datetime.now().isoformat()),
            }

            logger.info(f"  Batch data to insert: {batch_data}")
            response = self.supabase.client.table("batch").insert(batch_data).execute()

            if response.data:
                saved_batch = response.data[0]
                logger.info(f"âœ… Batch info saved successfully with enhanced concurrency control")
                logger.info(f"  Database ID: {saved_batch['id']}")
                logger.info(f"  Batch ID: {saved_batch['batch_id']}")
                logger.info(f"  Status: {saved_batch['status']}")
                logger.info(f"  Article count: {saved_batch['article_count']}")
                return saved_batch
            else:
                logger.error("âŒ Failed to save batch info - no data returned from database")
                return None

        except Exception as e:
            error_msg = str(e)
            logger.error(f"ğŸ’¥ Database operation failed while saving batch {batch_id}: {e}")

            # ë™ì‹œì„± ìœ„ë°˜ ì—ëŸ¬ ì²˜ë¦¬ (unique constraint violation)
            if "23505" in error_msg or "duplicate key value" in error_msg:
                logger.warning(f"ğŸ”„ Duplicate batch detected: {batch_id} was already created by another instance")
                logger.warning("This indicates a race condition was caught by database constraints")

                # ì´ë¯¸ ìƒì„±ëœ ë°°ì¹˜ ì •ë³´ ì¡°íšŒ
                try:
                    existing = self.supabase.client.table("batch").select("*").eq("batch_id", batch_id).execute()

                    if existing.data:
                        logger.info("ğŸ” Found existing batch created by concurrent instance")
                        logger.info(f"  Status: {existing.data[0]['status']}")
                        logger.info(f"  Created: {existing.data[0].get('created_at', 'unknown')}")
                        return existing.data[0]

                except Exception as lookup_error:
                    logger.error(f"âŒ Failed to lookup existing batch after duplicate error: {lookup_error}")

            # ê¸°íƒ€ ë°ì´í„°ë² ì´ìŠ¤ ì—ëŸ¬
            logger.error(f"âŒ Critical error saving batch {batch_id}: {e}")
            logger.error("This may require manual intervention to resolve")
            return None

    def update_batch_status(
        self, batch_id: str, status: str, error_message: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """
        ë°°ì¹˜ ìƒíƒœ ì—…ë°ì´íŠ¸ (ë©±ë“±ì„± ë³´ì¥)

        Args:
            batch_id: ë°°ì¹˜ ID
            status: ìƒˆë¡œìš´ ìƒíƒœ
            error_message: ì—ëŸ¬ ë©”ì‹œì§€ (ì„ íƒì‚¬í•­)

        Returns:
            ì—…ë°ì´íŠ¸ëœ ë°ì´í„° ë˜ëŠ” None
        """
        try:
            # 1ë‹¨ê³„: í˜„ì¬ ë°°ì¹˜ ìƒíƒœ í™•ì¸ (ë©±ë“±ì„± ì²´í¬)
            current_batch = self._get_batch_info(batch_id)

            if not current_batch:
                logger.warning(f"Batch {batch_id} not found for status update")
                return None

            current_status = current_batch.get("status")

            # ë©±ë“±ì„± ì²´í¬: ì´ë¯¸ ì›í•˜ëŠ” ìƒíƒœì¸ ê²½ìš°
            if current_status == status:
                logger.info(f"Batch {batch_id} is already in '{status}' status - operation is idempotent")
                return current_batch

            # ìƒíƒœ ì „ì´ ìœ íš¨ì„± ê²€ì¦
            if not self._is_valid_status_transition(current_status, status):
                logger.warning(f"Invalid status transition for batch {batch_id}: {current_status} -> {status}")
                return None

            # 2ë‹¨ê³„: ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤í–‰ (updated_at ì»¬ëŸ¼ ì œê±°ë¨)
            update_data = {
                "status": status,
            }

            # ì™„ë£Œ ê´€ë ¨ ìƒíƒœì¸ ê²½ìš° ì™„ë£Œ ì‹œê°„ ê¸°ë¡
            if status in ["completed", "failed", "cancelled"]:
                update_data["completed_at"] = datetime.now().isoformat()

            if error_message:
                update_data["error_message"] = error_message

            response = self.supabase.client.table("batch").update(update_data).eq("batch_id", batch_id).execute()

            if response.data:
                logger.info(f"Batch status updated successfully: {batch_id} ({current_status} -> {status})")
                return response.data[0]
            else:
                logger.error("Failed to update batch status - no data returned")
                return None

        except Exception as e:
            logger.error(f"Failed to update batch status: {e}")
            return None

    def _is_valid_status_transition(self, current_status: str, new_status: str) -> bool:
        """
        ë°°ì¹˜ ìƒíƒœ ì „ì´ ìœ íš¨ì„± ê²€ì¦

        Args:
            current_status: í˜„ì¬ ìƒíƒœ
            new_status: ìƒˆë¡œìš´ ìƒíƒœ

        Returns:
            ìœ íš¨í•œ ì „ì´ì¸ì§€ ì—¬ë¶€
        """
        # ìœ íš¨í•œ ìƒíƒœ ì „ì´ ê·œì¹™ ì •ì˜
        valid_transitions = {
            "in_progress": ["completed", "failed", "cancelled"],
            "completed": ["completed"],  # ì™„ë£Œëœ ìƒíƒœëŠ” ë‹¤ì‹œ ì™„ë£Œë¡œë§Œ ê°€ëŠ¥ (ë©±ë“±ì„±)
            "failed": ["failed", "in_progress"],  # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ì¬ì‹œë„ ê°€ëŠ¥
            "cancelled": ["cancelled"],  # ì·¨ì†Œëœ ìƒíƒœëŠ” ë‹¤ì‹œ ì·¨ì†Œë¡œë§Œ ê°€ëŠ¥ (ë©±ë“±ì„±)
        }

        allowed_next_statuses = valid_transitions.get(current_status, [])
        is_valid = new_status in allowed_next_statuses

        if not is_valid:
            logger.debug(f"Status transition validation: {current_status} -> {new_status} = {is_valid}")

        return is_valid

    def check_batch_completion(self, batch_id: str) -> Optional[str]:
        """
        OpenAI ë°°ì¹˜ ì™„ë£Œ ìƒíƒœ í™•ì¸

        Args:
            batch_id: ë°°ì¹˜ ID

        Returns:
            ë°°ì¹˜ ìƒíƒœ ë˜ëŠ” None
        """
        try:
            batch = self.openai_client.get_batch_status(batch_id)
            logger.debug(f"Batch {batch_id} status: {batch.status}")
            return batch.status

        except Exception as e:
            logger.error(f"Failed to check batch completion: {e}")
            return None

    def _get_batch_info(self, batch_id: str) -> Optional[Dict[str, Any]]:
        """
        ë°°ì¹˜ ì •ë³´ ì¡°íšŒ

        Args:
            batch_id: ë°°ì¹˜ ID

        Returns:
            ë°°ì¹˜ ì •ë³´ ë˜ëŠ” None
        """
        try:
            response = self.supabase.client.table("batch").select("*").eq("batch_id", batch_id).execute()

            if response.data:
                return response.data[0]
            return None

        except Exception as e:
            logger.error(f"Failed to get batch info: {e}")
            return None

    def _get_cached_or_download_results(self, batch_id: str) -> List[Dict[str, Any]]:
        """
        ìºì‹œëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ OpenAIì—ì„œ ë‹¤ìš´ë¡œë“œ

        Args:
            batch_id: ë°°ì¹˜ ID

        Returns:
            ë°°ì¹˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # TODO: í–¥í›„ Redisë‚˜ íŒŒì¼ ì‹œìŠ¤í…œ ìºì‹± êµ¬í˜„ ê°€ëŠ¥
        # í˜„ì¬ëŠ” ë§¤ë²ˆ ë‹¤ìš´ë¡œë“œí•˜ì§€ë§Œ, ë¡œê·¸ë¡œ ì¬ì‹¤í–‰ ìƒí™©ì„ ëª…í™•íˆ í‘œì‹œ

        try:
            # ì¬ì‹¤í–‰ ê°ì§€ ë¡œê¹…
            batch_info = self._get_batch_info(batch_id)
            if batch_info:
                created_at = batch_info.get("created_at", "unknown")
                logger.info(f"Re-processing batch created at: {created_at}")
                logger.info("Note: Results will be re-downloaded (caching not implemented yet)")

            # OpenAIì—ì„œ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
            return self.openai_client.get_batch_results(batch_id)

        except Exception as e:
            logger.error(f"Failed to get batch results: {e}")
            return []
