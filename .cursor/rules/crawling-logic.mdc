---
description: Crawling Logic
alwaysApply: false
---

# 네이버 뉴스 크롤링 로직

## 🔍 크롤링 방식

### 1. Naver Open API 크롤링 ✅ 구현완료
- **API 엔드포인트**: `https://openapi.naver.com/v1/search/news.json`
- **검색 키워드**: `["충격", "공포", "반전", "놀라운", "경악"]` (settings.py에서 관리)
- **필수 헤더**: `X-Naver-Client-Id`, `X-Naver-Client-Secret`
- **파라미터**: `query`, `display`, `start`, `sort`

### 2. HTML 파싱 크롤링 ✅ 구현완료
- **크롤링 방식**: BeautifulSoup을 사용한 HTML 파싱
- **추출 데이터**: 제목, 내용, 기자, 언론사, 발행시간, 네이버 URL
- **다중 선택자 지원**: 여러 뉴스 사이트 구조에 대응

## 📊 데이터 추출 규칙 ✅ 구현완료

### 공통 필드 매핑
```python
# src/models/article.py에서 구현됨
@dataclass
class Article:
    title: str                    # 기사 제목
    content: str                  # 기사 내용 (최대 700자)
    journalist_name: str          # 기자명
    publisher: str                # 언론사
    published_at: datetime        # 발행시간
    naver_url: str               # 네이버 뉴스 URL
    journalist_id: Optional[str] # 기자 ID (DB 저장 시 자동 설정)
    clickbait_score: Optional[int] # 낚시 점수 (0-100)
    clickbait_explanation: Optional[str] # 점수 설명
```

### 기자 정보 처리 ✅ 구현완료
- 기자명이 없는 경우 `익명기자_{언론사명}` 형태로 처리
- 기자 정보 캐싱으로 성능 최적화
- 언론사와 기자명 조합으로 유니크 체크
- `src/database/operations.py`의 `get_or_create_journalist()` 메서드에서 구현

## 🛠️ 구현 가이드라인 ✅ 구현완료

### 에러 핸들링
- **네트워크 타임아웃**: 30초 제한
- **API 호출 제한**: 키워드별 최대 100개 기사
- **파싱 실패**: 개별 뉴스 파싱 실패 시 로그 기록 후 다음 뉴스 처리
- **중복 처리**: 네이버 URL 기준 중복 체크 및 스킵

### 로깅 규칙 ✅ 구현완료
- 크롤링 시작/종료 시간 기록
- 처리된 기사 수 및 실패 수 기록
- 중복 기사 스킵 로그
- API 호출 실패 상세 로그
- 일별 로그 파일 자동 생성 (`logs/` 디렉토리)

### 데이터 검증 ✅ 구현완료
- 제목 최소 길이: 9자 (Article 모델에서 검증)
- 내용 최소 길이: 100자 (Article 모델에서 검증)
- 내용 최대 길이: 700자 (크롤러에서 자동 truncate)
- 네이버 URL 형식 검증
- 낚시 점수 범위 검증 (0-100)

## 🔄 크롤링 전략 ✅ 구현완료

### 배치 처리
- 날짜별 크롤링 (필수 파라미터로 변경)
- 날짜 검증: 최근 3개월 이내, 미래 날짜 제한
- 키워드별 병렬 처리
- 메모리 효율적인 배치 단위 처리

### 중복 방지 ✅ 구현완료
- 네이버 URL 기준 중복 체크 (`check_duplicate_article()`)
- 크롤링 전 중복 체크 옵션 (`check_duplicates` 파라미터)
- DB 삽입 전 최종 중복 검증

## 📈 성능 최적화 ✅ 구현완료

### 캐싱 전략
- 기자 정보 메모리 캐싱 (`journalist_cache`)
- 세션 재사용 (`requests.Session`)
- 배치 삽입으로 DB 호출 최소화

### 메모리 관리
- 기사 내용 700자 제한
- HTML 태그 및 엔티티 자동 정리
- 불필요한 데이터 즉시 정리

## 🚀 실행 방법

### 메인 실행
```bash
python main.py
```

### 스크립트 실행 (날짜 필수)
```bash
# 특정 날짜 크롤링
python scripts/crawl_news.py --date 2024-01-15

# 테스트 실행 (저장하지 않음)
python scripts/crawl_news.py --date 2024-01-15 --dry-run

# 키워드 및 개수 커스터마이징
python scripts/crawl_news.py --date 2024-01-15 --keywords 충격 반전 --max-per-keyword 50
```

## 📝 주요 구현 파일

### 핵심 크롤러 (`src/crawlers/naver_crawler.py`)
- `NaverNewsCrawler` 클래스
- `search_news_api()`: 네이버 API 검색
- `extract_article_content()`: HTML 파싱 및 내용 추출
- `crawl_by_keywords()`: 키워드별 크롤링
- `crawl_and_save()`: 크롤링 + DB 저장

### 실행 스크립트 (`scripts/crawl_news.py`)
- 날짜 파라미터 검증
- 명령행 인자 처리
- Dry-run 모드 지원
- 상세 로깅 및 에러 처리

### 설정 관리 (`src/config/settings.py`)
- 환경변수 관리
- 기본 키워드 설정
- 크롤링 제한 설정
- 설정 검증 (`validate()` 메서드)

## 🔍 향후 개선 계획 (미구현)

### 엔터테인먼트 뉴스 크롤링
- `https://m.entertain.naver.com/ranking/sympathy/surprise` 크롤링 추가
- `--source entertain` 파라미터 추가

### 확장 키워드 관리
- 키워드 확장 전략 구현
- 언론사별 특화 키워드
- 동적 키워드 업데이트

### 고급 중복 처리
- 제목 유사도 기반 중복 검사
- 시간 윈도우 기반 중복 제거
- 내용 해시값을 통한 중복 검사

- 이미 처리된 URL 캐싱
- 기자 정보 메모리 캐싱
- 언론사 정보 사전 로딩
