---
globs: "scripts/*"
description: "독립 실행 워크플로우 스크립트 가이드"
---

# 워크플로우 스크립트 가이드

## 📋 스크립트 구조

### 독립 실행 원칙
각 스크립트는 다음 조건을 만족해야 합니다:
- **독립성**: 다른 스크립트에 의존하지 않고 실행 가능
- **멱등성**: 여러 번 실행해도 안전한 결과
- **로깅**: 실행 과정과 결과를 상세히 기록
- **에러 핸들링**: 실패 시 적절한 복구 메커니즘

### 공통 구조 템플릿
```python
#!/usr/bin/env python3
"""
스크립트 설명
"""

import sys
import logging
import argparse
from datetime import datetime

# 로깅 설정
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/{__name__}_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def main():
    parser = argparse.ArgumentParser(description='스크립트 설명')
    parser.add_argument('--date', help='처리 날짜 (YYYY-MM-DD)')
    parser.add_argument('--dry-run', action='store_true', help='실제 실행 없이 테스트')
    
    args = parser.parse_args()
    
    try:
        logger.info(f"스크립트 시작: {__name__}")
        # 메인 로직
        logger.info("스크립트 완료")
    except Exception as e:
        logger.error(f"스크립트 실행 중 오류: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

## 🔄 주요 스크립트

### 1. crawl_news.py
```bash
python scripts/crawl_news.py --date 2024-01-15 --source api
python scripts/crawl_news.py --date 2024-01-15 --source entertain
```

**기능:**
- 지정된 날짜의 네이버 뉴스 크롤링
- 소스 선택 (api/entertain)
- 중복 체크 및 데이터 검증
- Supabase 저장

### 2. process_openai_batch.py
```bash
python scripts/process_openai_batch.py --create-batch
python scripts/process_openai_batch.py --batch-id batch_abc123
```

**기능:**
- 미처리 기사 조회
- OpenAI Batch API 요청 생성
- 배치 상태 추적

### 3. monitor_batches.py
```bash
python scripts/monitor_batches.py --check-all
python scripts/monitor_batches.py --batch-id batch_abc123
```

**기능:**
- 진행중인 배치 상태 확인
- 완료된 배치 알림
- 실패한 배치 재시도

### 4. process_completed_batches.py
```bash
python scripts/process_completed_batches.py --batch-id batch_abc123
python scripts/process_completed_batches.py --auto-process
```

**기능:**
- 완료된 배치 결과 다운로드
- 응답 파싱 및 점수 추출
- 기사 데이터 업데이트

## 📊 로깅 및 모니터링

### 로그 레벨 정의
- **DEBUG**: 상세한 디버깅 정보
- **INFO**: 일반적인 실행 정보
- **WARNING**: 경고 메시지
- **ERROR**: 오류 발생
- **CRITICAL**: 치명적인 오류

### 로그 포맷
```
2024-01-15 10:30:45 - crawl_news - INFO - 크롤링 시작: 2024-01-15
2024-01-15 10:30:47 - crawl_news - INFO - 처리된 기사 수: 150
2024-01-15 10:30:48 - crawl_news - WARNING - 중복 기사 스킵: 5개
2024-01-15 10:30:50 - crawl_news - INFO - 크롤링 완료: 2024-01-15
```

### 성능 지표 추적
```python
import time
from functools import wraps

def track_performance(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        execution_time = time.time() - start_time
        logger.info(f"{func.__name__} 실행 시간: {execution_time:.2f}초")
        return result
    return wrapper
```

## 🛠️ 에러 핸들링

### 재시도 메커니즘
```python
import time
import random

def retry_with_backoff(max_retries=3, base_delay=1, max_delay=60):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        logger.error(f"최종 실패: {e}")
                        raise
                    
                    delay = min(base_delay * (2 ** attempt) + random.uniform(0, 1), max_delay)
                    logger.warning(f"재시도 {attempt + 1}/{max_retries} in {delay:.2f}초: {e}")
                    time.sleep(delay)
            return None
        return wrapper
    return decorator
```

### 알림 시스템
```python
def send_notification(message, level='info'):
    """알림 발송 (슬랙, 이메일 등)"""
    if level == 'error':
        # 에러 알림 발송
        pass
    elif level == 'warning':
        # 경고 알림 발송
        pass
```

## 🔒 보안 및 설정

### 환경 변수 관리
```python
import os
from dotenv import load_dotenv

load_dotenv()

REQUIRED_ENV_VARS = [
    'SUPABASE_URL',
    'SUPABASE_KEY',
    'OPENAI_API_KEY',
    'NAVER_CLIENT_ID',
    'NAVER_CLIENT_SECRET'
]

def validate_environment():
    missing = [var for var in REQUIRED_ENV_VARS if not os.getenv(var)]
    if missing:
        logger.error(f"필수 환경변수 누락: {missing}")
        sys.exit(1)
```

### 설정 파일 검증
```python
def validate_config():
    """설정 파일 유효성 검사"""
    required_configs = ['batch_size', 'max_retries', 'timeout']
    # 검증 로직
```

## 📅 스케줄링

### Cron 설정 예시
```bash
# 매일 오전 9시 뉴스 크롤링
0 9 * * * /usr/bin/python3 /path/to/scripts/crawl_news.py --date $(date +\%Y-\%m-\%d)

# 매 10분마다 배치 모니터링
*/10 * * * * /usr/bin/python3 /path/to/scripts/monitor_batches.py --check-all

# 매 30분마다 완료된 배치 처리
*/30 * * * * /usr/bin/python3 /path/to/scripts/process_completed_batches.py --auto-process
```

### GitHub Actions 통합
```yaml
name: Daily News Crawling
on:
  schedule:
    - cron: '0 9 * * *'  # 매일 오전 9시
  workflow_dispatch:

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Crawl News
                 run: python scripts/crawl_news.py --date $(date +%Y-%m-%d)
```
