---
globs: "scripts/*"
description: "ë…ë¦½ ì‹¤í–‰ ì›Œí¬í”Œë¡œìš° ìŠ¤í¬ë¦½íŠ¸ ê°€ì´ë“œ"
---

# ì›Œí¬í”Œë¡œìš° ìŠ¤í¬ë¦½íŠ¸ ê°€ì´ë“œ

## ğŸ“‹ ìŠ¤í¬ë¦½íŠ¸ êµ¬ì¡°

### ë…ë¦½ ì‹¤í–‰ ì›ì¹™
ê° ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤:
- **ë…ë¦½ì„±**: ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ì‹¤í–‰ ê°€ëŠ¥
- **ë©±ë“±ì„±**: ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•´ë„ ì•ˆì „í•œ ê²°ê³¼
- **ë¡œê¹…**: ì‹¤í–‰ ê³¼ì •ê³¼ ê²°ê³¼ë¥¼ ìƒì„¸íˆ ê¸°ë¡
- **ì—ëŸ¬ í•¸ë“¤ë§**: ì‹¤íŒ¨ ì‹œ ì ì ˆí•œ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜

### ê³µí†µ êµ¬ì¡° í…œí”Œë¦¿
```python
#!/usr/bin/env python3
"""
ìŠ¤í¬ë¦½íŠ¸ ì„¤ëª…
"""

import sys
import logging
import argparse
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/{__name__}_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def main():
    parser = argparse.ArgumentParser(description='ìŠ¤í¬ë¦½íŠ¸ ì„¤ëª…')
    parser.add_argument('--date', help='ì²˜ë¦¬ ë‚ ì§œ (YYYY-MM-DD)')
    parser.add_argument('--dry-run', action='store_true', help='ì‹¤ì œ ì‹¤í–‰ ì—†ì´ í…ŒìŠ¤íŠ¸')
    
    args = parser.parse_args()
    
    try:
        logger.info(f"ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘: {__name__}")
        # ë©”ì¸ ë¡œì§
        logger.info("ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

## ğŸ”„ ì£¼ìš” ìŠ¤í¬ë¦½íŠ¸

### 1. crawl_news.py
```bash
python scripts/crawl_news.py --date 2024-01-15 --source api
python scripts/crawl_news.py --date 2024-01-15 --source entertain
```

**ê¸°ëŠ¥:**
- ì§€ì •ëœ ë‚ ì§œì˜ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
- ì†ŒìŠ¤ ì„ íƒ (api/entertain)
- ì¤‘ë³µ ì²´í¬ ë° ë°ì´í„° ê²€ì¦
- Supabase ì €ì¥

### 2. process_openai_batch.py
```bash
python scripts/process_openai_batch.py --create-batch
python scripts/process_openai_batch.py --batch-id batch_abc123
```

**ê¸°ëŠ¥:**
- ë¯¸ì²˜ë¦¬ ê¸°ì‚¬ ì¡°íšŒ
- OpenAI Batch API ìš”ì²­ ìƒì„±
- ë°°ì¹˜ ìƒíƒœ ì¶”ì 

### 3. monitor_batches.py
```bash
python scripts/monitor_batches.py --check-all
python scripts/monitor_batches.py --batch-id batch_abc123
```

**ê¸°ëŠ¥:**
- ì§„í–‰ì¤‘ì¸ ë°°ì¹˜ ìƒíƒœ í™•ì¸
- ì™„ë£Œëœ ë°°ì¹˜ ì•Œë¦¼
- ì‹¤íŒ¨í•œ ë°°ì¹˜ ì¬ì‹œë„

### 4. process_completed_batches.py
```bash
python scripts/process_completed_batches.py --batch-id batch_abc123
python scripts/process_completed_batches.py --auto-process
```

**ê¸°ëŠ¥:**
- ì™„ë£Œëœ ë°°ì¹˜ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
- ì‘ë‹µ íŒŒì‹± ë° ì ìˆ˜ ì¶”ì¶œ
- ê¸°ì‚¬ ë°ì´í„° ì—…ë°ì´íŠ¸

## ğŸ“Š ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§

### ë¡œê·¸ ë ˆë²¨ ì •ì˜
- **DEBUG**: ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´
- **INFO**: ì¼ë°˜ì ì¸ ì‹¤í–‰ ì •ë³´
- **WARNING**: ê²½ê³  ë©”ì‹œì§€
- **ERROR**: ì˜¤ë¥˜ ë°œìƒ
- **CRITICAL**: ì¹˜ëª…ì ì¸ ì˜¤ë¥˜

### ë¡œê·¸ í¬ë§·
```
2024-01-15 10:30:45 - crawl_news - INFO - í¬ë¡¤ë§ ì‹œì‘: 2024-01-15
2024-01-15 10:30:47 - crawl_news - INFO - ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: 150
2024-01-15 10:30:48 - crawl_news - WARNING - ì¤‘ë³µ ê¸°ì‚¬ ìŠ¤í‚µ: 5ê°œ
2024-01-15 10:30:50 - crawl_news - INFO - í¬ë¡¤ë§ ì™„ë£Œ: 2024-01-15
```

### ì„±ëŠ¥ ì§€í‘œ ì¶”ì 
```python
import time
from functools import wraps

def track_performance(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        execution_time = time.time() - start_time
        logger.info(f"{func.__name__} ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
        return result
    return wrapper
```

## ğŸ› ï¸ ì—ëŸ¬ í•¸ë“¤ë§

### ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
```python
import time
import random

def retry_with_backoff(max_retries=3, base_delay=1, max_delay=60):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        logger.error(f"ìµœì¢… ì‹¤íŒ¨: {e}")
                        raise
                    
                    delay = min(base_delay * (2 ** attempt) + random.uniform(0, 1), max_delay)
                    logger.warning(f"ì¬ì‹œë„ {attempt + 1}/{max_retries} in {delay:.2f}ì´ˆ: {e}")
                    time.sleep(delay)
            return None
        return wrapper
    return decorator
```

### ì•Œë¦¼ ì‹œìŠ¤í…œ
```python
def send_notification(message, level='info'):
    """ì•Œë¦¼ ë°œì†¡ (ìŠ¬ë™, ì´ë©”ì¼ ë“±)"""
    if level == 'error':
        # ì—ëŸ¬ ì•Œë¦¼ ë°œì†¡
        pass
    elif level == 'warning':
        # ê²½ê³  ì•Œë¦¼ ë°œì†¡
        pass
```

## ğŸ”’ ë³´ì•ˆ ë° ì„¤ì •

### í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬
```python
import os
from dotenv import load_dotenv

load_dotenv()

REQUIRED_ENV_VARS = [
    'SUPABASE_URL',
    'SUPABASE_KEY',
    'OPENAI_API_KEY',
    'NAVER_CLIENT_ID',
    'NAVER_CLIENT_SECRET'
]

def validate_environment():
    missing = [var for var in REQUIRED_ENV_VARS if not os.getenv(var)]
    if missing:
        logger.error(f"í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½: {missing}")
        sys.exit(1)
```

### ì„¤ì • íŒŒì¼ ê²€ì¦
```python
def validate_config():
    """ì„¤ì • íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
    required_configs = ['batch_size', 'max_retries', 'timeout']
    # ê²€ì¦ ë¡œì§
```

## ğŸ“… ìŠ¤ì¼€ì¤„ë§

### Cron ì„¤ì • ì˜ˆì‹œ
```bash
# ë§¤ì¼ ì˜¤ì „ 9ì‹œ ë‰´ìŠ¤ í¬ë¡¤ë§
0 9 * * * /usr/bin/python3 /path/to/scripts/crawl_news.py --date $(date +\%Y-\%m-\%d)

# ë§¤ 10ë¶„ë§ˆë‹¤ ë°°ì¹˜ ëª¨ë‹ˆí„°ë§
*/10 * * * * /usr/bin/python3 /path/to/scripts/monitor_batches.py --check-all

# ë§¤ 30ë¶„ë§ˆë‹¤ ì™„ë£Œëœ ë°°ì¹˜ ì²˜ë¦¬
*/30 * * * * /usr/bin/python3 /path/to/scripts/process_completed_batches.py --auto-process
```

### GitHub Actions í†µí•©
```yaml
name: Daily News Crawling
on:
  schedule:
    - cron: '0 9 * * *'  # ë§¤ì¼ ì˜¤ì „ 9ì‹œ
  workflow_dispatch:

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Crawl News
                 run: python scripts/crawl_news.py --date $(date +%Y-%m-%d)
```
